{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2-Style Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elilaird/CS83212-Advanced-Neural-Networks/blob/main/Lab2_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf5OYNnUIGrM"
      },
      "source": [
        "# Lab 2: Style Transfer\n",
        "\n",
        "**Group Members:**\n",
        "* Clay Harper\n",
        "* Eli Laird\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quM4QpEhKSp0",
        "outputId": "ecd52431-9cd6-498a-8e03-03242d957108"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(f'Tensorflow version: {tf.__version__}')\n",
        "print(f'Keras version: {keras.__version__}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models, Model, Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Conv2D, Input, UpSampling2D"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.4.1\n",
            "Keras version: 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaQOhCt5J23c"
      },
      "source": [
        "## VGG Manipulation\n",
        "\n",
        "Here, we need to manipulate the given VGG code (courtesy of Justin Ledford) to make use of pooling layers or strided convolutions alternatively.  We chose to use strided convolutions because it is less computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le6MnUcMIAy2",
        "outputId": "9dc8617f-db1a-4228-853a-a06c5366d6de"
      },
      "source": [
        "# Load VGG\n",
        "pre_trained_model = tf.keras.applications.VGG19(include_top=False,\n",
        "                                                      weights='imagenet')\n",
        "\n",
        "def vgg_layers(inputs, target_layer):\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
        "    if target_layer == 1:\n",
        "        return x\n",
        "    # Strides instead of maxpooling \n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', strides=2)(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    if target_layer == 2:\n",
        "        return x\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', strides=2)(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    if target_layer == 3:\n",
        "        return x\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4', strides=2)(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    if target_layer == 4:\n",
        "        return x\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4', strides=2)(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    return x\n",
        "\n",
        "def load_weights(trained_model, model):\n",
        "    layer_names = [layer.name for layer in trained_model.layers]\n",
        "\n",
        "    for layer in model.layers:\n",
        "        b_name = layer.name.encode()\n",
        "        if b_name in layer_names:\n",
        "            layer.set_weights(trained_model.get_layer(b_name).get_weights())\n",
        "            layer.trainable = False\n",
        "\n",
        "def VGG19(trained_model, input_tensor=None, input_shape=None, target_layer=1):\n",
        "    \"\"\"\n",
        "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
        "    \"\"\"\n",
        "    if input_tensor is None:\n",
        "        inputs = Input(shape=input_shape)\n",
        "    else:\n",
        "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
        "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
        "    load_weights(trained_model, model)\n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjDwH6xSTBMW"
      },
      "source": [
        "Create an encoder network from the pretrained VGG network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhDJoX1bIGcL",
        "outputId": "57e7afeb-0834-433d-80a3-21a34387fd19"
      },
      "source": [
        "target_layer = 3\n",
        "vgg_model = VGG19(pre_trained_model, input_shape=(300, 300, 3), target_layer=target_layer)\n",
        "vgg_model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n",
            "=================================================================\n",
            "Total params: 555,328\n",
            "Trainable params: 555,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur4iav2GTkyH"
      },
      "source": [
        "## Decoder Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aMf8aNhTUsc"
      },
      "source": [
        "def decoder_layers(inputs, layer):\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
        "    if layer == 5:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
        "    x = Conv2D(512, (4, 4), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
        "    if layer == 4:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
        "    if layer == 3:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
        "    if layer == 2:\n",
        "        return x\n",
        "\n",
        "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
        "    if layer == 1:\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96q2OyoLUqIq"
      },
      "source": [
        "## Encoder-Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1zyMow9UmVL"
      },
      "source": [
        "LAMBDA=1\n",
        "\n",
        "def l2_loss(x):\n",
        "  return K.sum(K.square(x)) / 2\n",
        "\n",
        "class EncoderDecoder:\n",
        "  def __init__(self, trained_model, input_shape=(256, 256, 3), target_layer=5,\n",
        "                decoder_path=None):\n",
        "    self.input_shape = input_shape\n",
        "    self.target_layer = target_layer\n",
        "    self.trained_model = trained_model\n",
        "\n",
        "    self.encoder = VGG19(trained_model, input_shape=input_shape, target_layer=target_layer)\n",
        "    if decoder_path:\n",
        "      self.decoder = load_model(decoder_path)\n",
        "    else:\n",
        "      self.decoder = self.create_decoder(target_layer)\n",
        "\n",
        "    self.model = Sequential()\n",
        "    self.model.add(self.encoder)\n",
        "    self.model.add(self.decoder)\n",
        "\n",
        "    self.loss = self.create_loss_fn(self.encoder)\n",
        "\n",
        "    self.model.compile('adam', self.loss)\n",
        "\n",
        "  def create_loss_fn(self, encoder):\n",
        "    def get_encodings(inputs):\n",
        "      encoder = VGG19(trained_model, inputs, self.input_shape, self.target_layer)\n",
        "      return encoder.output\n",
        "\n",
        "    def loss(img_in, img_out):\n",
        "      encoding_in = get_encodings(img_in)\n",
        "      encoding_out = get_encodings(img_out)\n",
        "      return l2_loss(img_out - img_in) + \\\n",
        "              LAMBDA*l2_loss(encoding_out - encoding_in)\n",
        "    return loss\n",
        "\n",
        "  def summary(self):\n",
        "    self.model.summary()\n",
        "\n",
        "  def create_decoder(self, target_layer):\n",
        "    inputs = Input(shape=self.encoder.output_shape[1:])\n",
        "    layers = decoder_layers(inputs, target_layer)\n",
        "    output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
        "                    name='decoder_out')(layers)\n",
        "    return Model(inputs, output, name='decoder_%s' % target_layer)\n",
        "\n",
        "  def export_decoder(self):\n",
        "    self.decoder.save('decoder_%s.h5' % self.target_layer)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFOPUQs9WM89"
      },
      "source": [
        "## Train Two Decoders \n",
        "\n",
        "Decoders will be created based on the outputs of 2 different layers in the encoder model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFCt6eYuVEWS",
        "outputId": "c8b7765e-f9c9-4585-a9ba-c678256aefca"
      },
      "source": [
        "encoder_decoder = EncoderDecoder(pre_trained_model, target_layer=target_layer)\n",
        "encoder_decoder.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg19 (Functional)           (None, 64, 64, 256)       555328    \n",
            "_________________________________________________________________\n",
            "decoder_3 (Functional)       (None, 256, 256, 3)       15411459  \n",
            "=================================================================\n",
            "Total params: 15,966,787\n",
            "Trainable params: 15,966,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp2COm9zVIml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6e5b9d6e-71e1-492c-ce0d-3b8b102fa752"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import Callback\n",
        "#from scipy.misc import imresize, imsave DEPRACATED\n",
        "from cv2 import resize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#from model import EncoderDecoder\n",
        "#from util import count_num_samples\n",
        "\n",
        "TRAIN_PATH = 'data'\n",
        "TARGET_SIZE = (256, 256)\n",
        "BATCH_SIZE = 4\n",
        "epochs = 2\n",
        "\n",
        "datagen = ImageDataGenerator()\n",
        "gen = datagen.flow_from_directory(TRAIN_PATH, target_size=TARGET_SIZE,\n",
        "                                  batch_size=BATCH_SIZE, class_mode=None)\n",
        "\n",
        "\n",
        "def create_gen(img_dir, target_size, batch_size):\n",
        "    datagen = ImageDataGenerator()\n",
        "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
        "                                      batch_size=batch_size, class_mode=None)\n",
        "\n",
        "    def tuple_gen():\n",
        "        for img in gen:\n",
        "            if img.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            # (X, y)\n",
        "            yield (img, img)\n",
        "\n",
        "    return tuple_gen()\n",
        "\n",
        "# This needs to be in scope where model is defined\n",
        "class OutputPreview(Callback):\n",
        "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
        "        test_img = image.load_img(test_img_path)\n",
        "        test_img = resize(src=test_img, dsize=(256,256,3)) #imresize(test_img, (256, 256, 3))\n",
        "        test_target = image.img_to_array(test_img)\n",
        "        test_target = np.expand_dims(test_target, axis=0)\n",
        "        self.test_img = test_target\n",
        "        self.model = model\n",
        "\n",
        "        self.preview_dir_path = preview_dir_path\n",
        "\n",
        "        self.increment = increment\n",
        "        self.iteration = 0\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        if (self.iteration % self.increment == 0):\n",
        "            output_img = self.model.predict(self.test_img)[0]\n",
        "            fname = '%d.jpg' % self.iteration\n",
        "            out_path = os.path.join(self.preview_dir_path, fname)\n",
        "            imsave(out_path, output_img)\n",
        "\n",
        "        self.iteration += 1\n",
        "\n",
        "\n",
        "gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
        "\n",
        "num_samples = 1 #count_num_samples(TRAIN_PATH)\n",
        "steps_per_epoch = num_samples // BATCH_SIZE\n",
        "\n",
        "target_layer = 1 #int(sys.argv[1])\n",
        "\n",
        "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
        "\n",
        "callbacks = [OutputPreview(encoder_decoder, './doge.jpg', 5000, './preview-%d' % target_layer)]\n",
        "encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs, callbacks=callbacks)\n",
        "encoder_decoder.export_decoder()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c2e89c1fcfcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtarget_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#int(sys.argv[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mOutputPreview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./doge.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./preview-%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'trained_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0QDOgptL6jG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}