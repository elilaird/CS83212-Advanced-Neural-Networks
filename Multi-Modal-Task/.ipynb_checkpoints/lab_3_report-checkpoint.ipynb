{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f6da76",
   "metadata": {},
   "source": [
    "# Lab 3: Multi-modal and Multi-task\n",
    "\n",
    "**Group Members:**\n",
    "* Clay Harper\n",
    "* Eli Laird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f417b48",
   "metadata": {},
   "source": [
    "In this report, we were to choose a dataset where a multi-modal model (multiple input streams), multi-task model (multiple predictive tasks), or both could be created.  We decided to use the 2018 OpenMic dataset because some of Clay's research is based in audio processing, and Eli has in interest in breaking into this field.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89f724",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The 2018 OpenMic dataset [1] was created in a collaboration effort between Spotify and MARL@NYU (Music and Audio Research Laboratory New York University).\n",
    "\n",
    "### Classification Task\n",
    "\n",
    "The classification task is to determine what instruments are present in the audio clip.  Each audio clip may contain multiple instruments, so the task is to determine all instruments present in the clip.  The classification labels are:\n",
    "\n",
    "* accordion\n",
    "* banjo\n",
    "* bass\n",
    "* cello\n",
    "* clarinet\n",
    "* cymbals\n",
    "* drums\n",
    "* flute\n",
    "* guitar\n",
    "* mallet_percussion\n",
    "* mandolin\n",
    "* organ\n",
    "* piano\n",
    "* saxophone,\n",
    "* synthesizer\n",
    "* trombone\n",
    "* trumpet\n",
    "* ukulele\n",
    "* violin\n",
    "* voice\n",
    "\n",
    "Each audio clip has been labeled by a variety of people on which instruments are present in the clip.  Each person, depending on their area of expertise, is given a `worker_trust` score--so a piano player will get a high trust on identifying if there is a piano but maybe a lower trust in identifying a ukulele.  From that, a `relevance_score` is computed by a weighted sum of the `worker_trust` and the `label` (1 or 0) if the instrument was present.  Essentially, each audio clip is given a `relevance_score` for each instrument.  `relevance_score` is basically a confidence score for how likely this instrument appears in the audio clip.\n",
    "\n",
    "### Feature Data Format\n",
    "\n",
    "There are multiple options we can do for the feature data.  The data archive includes both raw audio files in the form of `.ogg` files and pre-computed VGGish Features [3].  If we wanted, we could featurize of the `.ogg` files by using the raw amplitude values, use MFCCs, CQT, etc.  For simplicity in this lab, we have decided to just use the VGGish features provided.   \n",
    "\n",
    "\n",
    "### Mulit-Modal/Multi-Task/Both?\n",
    "\n",
    "We thought about making this project be both multi-modal and multi-task, but since we are using the VGGish Features, we decided to just make this project muli-task where the tasks are identifying each instrument in the audio clip separately.\n",
    "\n",
    "### Who Collected the Data?\n",
    "\n",
    "Spotify and MARL@NYU (Music and Audio Research Laboratory New York University).  The cost of annotation was sponsored by Spotify.\n",
    "\n",
    "#### Why was the Data Collected?\n",
    "\n",
    "The idea was to create a dataset that can be used in music information retrieval through identifying different instruments in an audio clip.  Some applications of music information retrieval are music genre classification, recommender systems, music separation, automatic music transcription, music generation, and more [2]. To give an example of how this dataset could help Spotify, think of a user who wants to listen to piano music on a long day.  The user could go to the search bar on Spotify and type in piano.  In order to get good results, the Spotify must have piano tags associated with songs to return piano music.  This process of tagging can be very labor intensive and expensive because people have to listen to a song, identify it as piano music, and tag the song in the database.  This is increasingly difficult when more songs are constantly added to Spotify's database.  Instead, if we can create a model that is very good at listening to music and segmenting out the types of instruments in the music, we can help automate this process (also VGGish was developed by Google so...).  VGGish features are computed using a pre-trained CNN from Google based on [3], which essentially uses a very similar architecture to the VGG image classification architecture.  In total, there are 17 layers containing convolutional layers, activations, followed by maxpooling operations.  In the OpenMic version, VGGish Features are computed on an embedding layer in the VGGish archtitecture and then projected down to a _____ dimensional space using PCA. \n",
    "\n",
    "#### When was the Data Collected?\n",
    "\n",
    "The data was collected and put together in 2018.\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "We thought about discretizing the `relevance_score` by some threshold (say .5) and making this a binary classification problem for each instrument.  Instead, we decided to regress the `relevance_score` for each instrument because this allows for more post-processing, particularly in an example discussed above with a user searching for piano music.  If we used a threshold of .5, a `relevance_score` of .51 would mean that, yes, a piano appears in this audio clip.  Well, maybe the piano just appears for a breif amount of time in the audio clip and that's why it had a relatively low confidence score.  The user searching for piano music would likely want songs that are very piano-based--that's why they probably searched `piano`.  Regressing the relevance score instead would allow better post-processing where maybe we can sort the search results from highest to lowest `relevance_score` so the user is happier with the search results.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "[1] Humphrey, Eric J., Durand, Simon, and McFee, Brian. \"OpenMIC-2018: An Open Dataset for Multiple Instrument Recognition.\" in Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), 2018.\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Music_information_retrieval#:~:text=Music%20information%20retrieval%20(MIR)%20is,with%20many%20real%2Dworld%20applications.\n",
    "\n",
    "[3] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron Weiss, & Kevin Wilson (2017). CNN Architectures for Large-Scale Audio Classification. In International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df74a1",
   "metadata": {},
   "source": [
    "## How many tasks or modalities are there in the dataset and how do you define each task or modality?\n",
    "\n",
    "There are 20 tasks (1 for each instrument), and there is 1 modality (the VGGish Feature).  \n",
    "\n",
    "****domains/cross domains\n",
    "\n",
    "**BEWARE**\n",
    "* TF2.0.0 isn't compatable with python > 3.7\n",
    "* Must use TF2.0.0 for maneframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269e578",
   "metadata": {},
   "source": [
    "## Load In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc43efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data files are: ['X', 'Y_true', 'Y_mask', 'sample_key']\n",
      "Data shape: (20000, 10, 128)\n",
      "True label shape: (20000, 20)\n",
      "True label mask shape: (20000, 20)\n",
      "Total samples: (20000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accordion': 0,\n",
       " 'banjo': 1,\n",
       " 'bass': 2,\n",
       " 'cello': 3,\n",
       " 'clarinet': 4,\n",
       " 'cymbals': 5,\n",
       " 'drums': 6,\n",
       " 'flute': 7,\n",
       " 'guitar': 8,\n",
       " 'mallet_percussion': 9,\n",
       " 'mandolin': 10,\n",
       " 'organ': 11,\n",
       " 'piano': 12,\n",
       " 'saxophone': 13,\n",
       " 'synthesizer': 14,\n",
       " 'trombone': 15,\n",
       " 'trumpet': 16,\n",
       " 'ukulele': 17,\n",
       " 'violin': 18,\n",
       " 'voice': 19}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with np.load('./openmic-2018/openmic-2018.npz', allow_pickle=True) as data:\n",
    "    files = data.files\n",
    "    print(f'The data files are: {files}')\n",
    "    \n",
    "    x_total = data['X']\n",
    "    y = data['Y_true']\n",
    "#     y[y == .5] = 0 # weird dataset (.5 only occurs with their 50/50 shot--not ever a relevance score)\n",
    "    # Call it a soft-no since it appears in the individual df but not in the label df\n",
    "    y_mask = data['Y_mask']\n",
    "    sample_keys = data['sample_key']\n",
    "\n",
    "print(f'Data shape: {x_total.shape}')\n",
    "print(f'True label shape: {y.shape}')\n",
    "print(f'True label mask shape: {y_mask.shape}')\n",
    "print(f'Total samples: {sample_keys.shape}')\n",
    "\n",
    "with open('./openmic-2018/class-map.json', 'r') as f:\n",
    "    class_map = json.load(f)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa979f7d",
   "metadata": {},
   "source": [
    "### Split Into Train/Test\n",
    "\n",
    "**We can talk about this more later, could do 80/20 split-probably should do cross validation**\n",
    "\n",
    "**Using their split for now**\n",
    "\n",
    "**Well,...they have their own split.  Not sure if this is what Dr. Larson wants though**\n",
    "\n",
    "* Should be useful:\n",
    "    * https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idexes = list(range(len(sample_keys)))\n",
    "k_folds = 5\n",
    "# Use for later when training datasets\n",
    "splitter = ShuffleSplit(n_splits=5, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9ce02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 14915 test samples: 5085\n"
     ]
    }
   ],
   "source": [
    "# If we use their split\n",
    "split_train = pd.read_csv('./openmic-2018/partitions/split01_train.csv', \n",
    "                          header=None, squeeze=True)\n",
    "split_test = pd.read_csv('./openmic-2018/partitions/split01_test.csv', \n",
    "                         header=None, squeeze=True)\n",
    "\n",
    "print(f'Train samples: {len(split_train)} test samples: {len(split_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b953af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key_df = pd.DataFrame({'sample_key': sample_keys})\n",
    "sample_key_df['index'] = sample_key_df.index\n",
    "train_keys_df = pd.DataFrame({'sample_key': split_train})\n",
    "test_keys_df = pd.DataFrame({'sample_key': split_test})\n",
    "\n",
    "# Get the train and test indexes according to thier split\n",
    "train_idxs = np.array(train_keys_df.merge(sample_key_df, on='sample_key', how='left')['index'])\n",
    "test_idxs = np.array(test_keys_df.merge(sample_key_df, on='sample_key', how='left')['index'])\n",
    "\n",
    "# Split the train/test data\n",
    "x_train, x_test = x_total[train_idxs], x_total[test_idxs]\n",
    "y_train, y_test = y[train_idxs], y[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4243c",
   "metadata": {},
   "source": [
    "**Insert architecture here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebbf325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 128)]         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               640500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 792,870\n",
      "Trainable params: 792,870\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, Input, Layer, GlobalAveragePooling1D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def create_nn():\n",
    "    inputs = Input(shape=(10, 128))\n",
    "    net = Flatten()(inputs)\n",
    "    net = Dense(500, activation='relu')(net)\n",
    "    net = Dense(250, activation='relu')(net)\n",
    "    net = Dense(100, activation='relu')(net)\n",
    "    output = Dense(20, activation='linear')(net)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    adam = Adam(1e-4)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "create_nn().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b13881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling1d (Glob (None, 128)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_variance_pooling1d[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 500)          128500      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 250)          125250      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          25100       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           2020        dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 280,870\n",
      "Trainable params: 280,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GlobalVariancePooling1D(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return K.var(x, axis=1)\n",
    "\n",
    "def create_nn_x_vec():\n",
    "    inputs = Input(shape=(10, 128))\n",
    "    gap = GlobalAveragePooling1D()(inputs)\n",
    "    gvp = GlobalVariancePooling1D()(inputs)\n",
    "    net = concatenate([gap, gvp])\n",
    "    net = Dense(500, activation='relu')(net)\n",
    "    net = Dense(250, activation='relu')(net)\n",
    "    net = Dense(100, activation='relu')(net)\n",
    "    output = Dense(20, activation='linear')(net)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    adam = Adam(1e-4)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "create_nn_x_vec().summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e8fad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab9fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.data.Dataset import from_tensor_slices\n",
    "batch_size = 16\n",
    "\n",
    "# Convert to tf data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(100).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).cache().shuffle(100).batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49864d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 935 steps, validate for 330 steps\n",
      "Epoch 1/1000\n",
      "935/935 [==============================] - 4s 4ms/step - loss: 115.1479 - mse: 115.2391 - mae: 6.3939 - val_loss: 16.7181 - val_mse: 16.7188 - val_mae: 3.1866\n",
      "Epoch 2/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 6.0316 - mse: 6.0366 - mae: 1.5782 - val_loss: 0.3487 - val_mse: 0.3487 - val_mae: 0.5170\n",
      "Epoch 3/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.2754 - mse: 0.2755 - mae: 0.4852 - val_loss: 0.2501 - val_mse: 0.2501 - val_mae: 0.4692\n",
      "Epoch 4/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.2249 - mse: 0.2250 - mae: 0.4499 - val_loss: 0.2087 - val_mse: 0.2087 - val_mae: 0.4335\n",
      "Epoch 5/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.1864 - mse: 0.1864 - mae: 0.4111 - val_loss: 0.1681 - val_mse: 0.1681 - val_mae: 0.3897\n",
      "Epoch 6/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.1466 - mse: 0.1466 - mae: 0.3639 - val_loss: 0.1285 - val_mse: 0.1285 - val_mae: 0.3393\n",
      "Epoch 7/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.1088 - mse: 0.1088 - mae: 0.3115 - val_loss: 0.0927 - val_mse: 0.0927 - val_mae: 0.2854\n",
      "Epoch 8/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0764 - mse: 0.0764 - mae: 0.2569 - val_loss: 0.0640 - val_mse: 0.0640 - val_mae: 0.2310\n",
      "Epoch 9/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0519 - mse: 0.0519 - mae: 0.2034 - val_loss: 0.0437 - val_mse: 0.0437 - val_mae: 0.1793\n",
      "Epoch 10/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0357 - mse: 0.0357 - mae: 0.1543 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1334\n",
      "Epoch 11/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1127 - val_loss: 0.0247 - val_mse: 0.0247 - val_mae: 0.0966\n",
      "Epoch 12/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.0820 - val_loss: 0.0228 - val_mse: 0.0228 - val_mae: 0.0725\n",
      "Epoch 13/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.0654 - val_loss: 0.0217 - val_mse: 0.0217 - val_mae: 0.0619\n",
      "Epoch 14/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.0605 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0606\n",
      "Epoch 15/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0599 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0603\n",
      "Epoch 16/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0601\n",
      "Epoch 17/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0598 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0602\n",
      "Epoch 18/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0603\n",
      "Epoch 19/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0598 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0603\n",
      "Epoch 20/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0600 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0602\n",
      "Epoch 21/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0600 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0601\n",
      "Epoch 22/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0599 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0601\n",
      "Epoch 23/1000\n",
      "935/935 [==============================] - 3s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0598 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0601\n",
      "=====================================================================================================================\n",
      "Results:\n",
      "MSE: 0.02\tMAE: 0.060\n"
     ]
    }
   ],
   "source": [
    "model = create_nn()\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='./models/mlp.h5'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=.1, patience=5, min_lr=1e-6),\n",
    "    EarlyStopping(patience=7)\n",
    "]\n",
    "\n",
    "train_steps_per_epoch = len(x_train)//batch_size + len(x_train)%batch_size\n",
    "val_steps_per_epoch = len(x_test)//batch_size + len(x_test)%batch_size\n",
    "model.fit(train_dataset, validation_data=test_dataset,\n",
    "                    epochs=1000, callbacks=callbacks,\n",
    "                    steps_per_epoch=train_steps_per_epoch,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    verbose=1)\n",
    "\n",
    "print('='*117)\n",
    "print('Results:')\n",
    "\n",
    "_, mse, mae = model.evaluate(test_dataset, steps=val_steps_per_epoch, verbose=0)\n",
    "print(f'MSE: {mse:.2f}\\tMAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7529fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 935 steps, validate for 330 steps\n",
      "Epoch 1/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 14967.6204 - mse: 14980.5889 - mae: 54.5322 - val_loss: 44.7848 - val_mse: 44.8092 - val_mae: 2.0801\n",
      "Epoch 2/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 18.5767 - mse: 18.5926 - mae: 1.2146 - val_loss: 9.2180 - val_mse: 9.2231 - val_mae: 0.8276\n",
      "Epoch 3/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 3.8687 - mse: 3.8718 - mae: 0.6681 - val_loss: 4.6060 - val_mse: 4.6085 - val_mae: 0.6490\n",
      "Epoch 4/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 1.1773 - mse: 1.1781 - mae: 0.5503 - val_loss: 3.2636 - val_mse: 3.2653 - val_mae: 0.5901\n",
      "Epoch 5/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.5187 - mse: 0.5189 - mae: 0.5063 - val_loss: 2.6178 - val_mse: 2.6182 - val_mae: 0.5588\n",
      "Epoch 6/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.3055 - mse: 0.3056 - mae: 0.4811 - val_loss: 2.3047 - val_mse: 2.3059 - val_mae: 0.5365\n",
      "Epoch 7/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.2439 - mse: 0.2440 - mae: 0.4617 - val_loss: 2.2026 - val_mse: 2.2037 - val_mae: 0.5161\n",
      "Epoch 8/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.2129 - mse: 0.2129 - mae: 0.4395 - val_loss: 2.1133 - val_mse: 2.1144 - val_mae: 0.4902\n",
      "Epoch 9/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.1859 - mse: 0.1859 - mae: 0.4108 - val_loss: 2.0313 - val_mse: 2.0324 - val_mae: 0.4549\n",
      "Epoch 10/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.1548 - mse: 0.1549 - mae: 0.3734 - val_loss: 1.9830 - val_mse: 1.9841 - val_mae: 0.4133\n",
      "Epoch 11/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.1210 - mse: 0.1210 - mae: 0.3283 - val_loss: 1.9223 - val_mse: 1.9233 - val_mae: 0.3651\n",
      "Epoch 12/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0896 - mse: 0.0896 - mae: 0.2782 - val_loss: 1.8824 - val_mse: 1.8834 - val_mae: 0.3140\n",
      "Epoch 13/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0632 - mse: 0.0632 - mae: 0.2265 - val_loss: 1.8639 - val_mse: 1.8649 - val_mae: 0.2629\n",
      "Epoch 14/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0438 - mse: 0.0438 - mae: 0.1767 - val_loss: 1.8248 - val_mse: 1.8258 - val_mae: 0.2154\n",
      "Epoch 15/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0315 - mse: 0.0315 - mae: 0.1325 - val_loss: 1.8150 - val_mse: 1.8160 - val_mae: 0.1755\n",
      "Epoch 16/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.0972 - val_loss: 1.8108 - val_mse: 1.8119 - val_mae: 0.1462\n",
      "Epoch 17/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.0739 - val_loss: 1.8091 - val_mse: 1.8101 - val_mae: 0.1299\n",
      "Epoch 18/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.0638 - val_loss: 1.8086 - val_mse: 1.8096 - val_mae: 0.1251\n",
      "Epoch 19/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0610 - val_loss: 1.8085 - val_mse: 1.8095 - val_mae: 0.1236\n",
      "Epoch 20/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0601 - val_loss: 1.8079 - val_mse: 1.8089 - val_mae: 0.1228\n",
      "Epoch 21/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 1.8079 - val_mse: 1.8089 - val_mae: 0.1227\n",
      "Epoch 22/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 1.8079 - val_mse: 1.8089 - val_mae: 0.1228\n",
      "Epoch 23/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 1.8085 - val_mse: 1.8095 - val_mae: 0.1230\n",
      "Epoch 24/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 1.8079 - val_mse: 1.8089 - val_mae: 0.1228\n",
      "Epoch 25/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0597 - val_loss: 1.8079 - val_mse: 1.8089 - val_mae: 0.1228\n",
      "Epoch 26/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0601 - val_loss: 1.8085 - val_mse: 1.8095 - val_mae: 0.1230\n",
      "Epoch 27/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0600 - val_loss: 1.8085 - val_mse: 1.8095 - val_mae: 0.1229\n",
      "Epoch 28/1000\n",
      "935/935 [==============================] - 2s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0599 - val_loss: 1.8085 - val_mse: 1.8095 - val_mae: 0.1229\n",
      "=====================================================================================================================\n",
      "Results:\n",
      "MSE: 1.81\tMAE: 0.123\n"
     ]
    }
   ],
   "source": [
    "model = create_nn_x_vec()\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='./models/x_vec_dense.h5'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=.1, patience=5, min_lr=1e-6),\n",
    "    EarlyStopping(patience=7)\n",
    "]\n",
    "\n",
    "train_steps_per_epoch = len(x_train)//batch_size + len(x_train)%batch_size\n",
    "val_steps_per_epoch = len(x_test)//batch_size + len(x_test)%batch_size\n",
    "model.fit(train_dataset, validation_data=test_dataset,\n",
    "                    epochs=1000, callbacks=callbacks,\n",
    "                    steps_per_epoch=train_steps_per_epoch,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    verbose=1)\n",
    "\n",
    "print('='*117)\n",
    "print('Results:')\n",
    "\n",
    "_, mse, mae = model.evaluate(test_dataset, steps=val_steps_per_epoch, verbose=0)\n",
    "print(f'MSE: {mse:.2f}\\tMAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138af15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f83db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
