{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431f5668",
   "metadata": {},
   "source": [
    "# Lab 3: Multi-modal and Multi-task\n",
    "\n",
    "**Group Members:**\n",
    "* Clay Harper\n",
    "* Eli Laird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa0161",
   "metadata": {},
   "source": [
    "In this report, we were to choose a dataset where a multi-modal model (multiple input streams), multi-task model (multiple predictive tasks), or both could be created.  We decided to use the 2018 OpenMic dataset because some of Clay's research is based in audio processing, and Eli has in interest in breaking into this field.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac710c",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The 2018 OpenMic dataset [1] was created in a collaboration effort between Spotify and MARL@NYU (Music and Audio Research Laboratory New York University).\n",
    "\n",
    "### Classification Task\n",
    "\n",
    "The classification task is to determine what instruments are present in the audio clip.  Each audio clip may contain multiple instruments, so the task is to determine all instruments present in the clip.  The classification labels are:\n",
    "\n",
    "* clarinet\t\t\t\n",
    "* flute\t\t\t\n",
    "* trumpet\t\t\t\n",
    "* saxophone\t\t\t\n",
    "* voice\t\t\t\n",
    "* accordion\t\t\t\n",
    "* ukulele\t\t\t\n",
    "* mallet_percussion\t\t\t\n",
    "* piano\t\t\t\n",
    "* guitar\n",
    "\n",
    "Each audio clip has been labeled by a variety of people on which instruments are present in the clip.  Each person, depending on their area of expertise, is given a `worker_trust` score--so a piano player will get a high trust on identifying if there is a piano but maybe a lower trust in identifying a ukulele.  From that, a `relevance_score` is computed by a weighted sum of the `worker_trust` and the `label` (1 or 0) if the instrument was present.  Essentially, each audio clip is given a `relevance_score` for each instrument.  `relevance_score` is basically a confidence score for how likely this instrument appears in the audio clip.\n",
    "\n",
    "### Feature Data Format\n",
    "\n",
    "There are multiple options we can do for the feature data.  The data archive includes both raw audio files in the form of `.ogg` files and pre-computed VGGish Features [3].  If we wanted, we could featurize of the `.ogg` files by using the raw amplitude values, use MFCCs, CQT, etc.  For simplicity in this lab, we have decided to just use the VGGish features provided.   \n",
    "\n",
    "\n",
    "### Mulit-Modal/Multi-Task/Both?\n",
    "\n",
    "We thought about making this project be both multi-modal and multi-task, but since we are using the VGGish Features, we decided to just make this project muli-task where the tasks are identifying each instrument in the audio clip separately.\n",
    "\n",
    "### Who Collected the Data?\n",
    "\n",
    "Spotify and MARL@NYU (Music and Audio Research Laboratory New York University).  The cost of annotation was sponsored by Spotify.\n",
    "\n",
    "#### Why was the Data Collected?\n",
    "\n",
    "The idea was to create a dataset that can be used in music information retrieval through identifying different instruments in an audio clip.  Some applications of music information retrieval are music genre classification, recommender systems, music separation, automatic music transcription, music generation, and more [2]. To give an example of how this dataset could help Spotify, think of a user who wants to listen to piano music on a long day.  The user could go to the search bar on Spotify and type in piano.  In order to get good results, the Spotify must have piano tags associated with songs to return piano music.  This process of tagging can be very labor intensive and expensive because people have to listen to a song, identify it as piano music, and tag the song in the database.  This is increasingly difficult when more songs are constantly added to Spotify's database.  Instead, if we can create a model that is very good at listening to music and segmenting out the types of instruments in the music, we can help automate this process (also VGGish was developed by Google so...).  VGGish features are computed using a pre-trained CNN from Google based on [3], which essentially uses a very similar architecture to the VGG image classification architecture.  In total, there are 17 layers containing convolutional layers, activations, followed by maxpooling operations.  In the OpenMic version, VGGish Features are computed on an embedding layer in the VGGish archtitecture and then projected down to a _____ dimensional space using PCA. \n",
    "\n",
    "#### When was the Data Collected?\n",
    "\n",
    "The data was collected and put together in 2018.\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "We thought about discretizing the `relevance_score` by some threshold (say .5) and making this a binary classification problem for each instrument.  Instead, we decided to regress the `relevance_score` for each instrument because this allows for more post-processing, particularly in an example discussed above with a user searching for piano music.  If we used a threshold of .5, a `relevance_score` of .51 would mean that, yes, a piano appears in this audio clip.  Well, maybe the piano just appears for a breif amount of time in the audio clip and that's why it had a relatively low confidence score.  The user searching for piano music would likely want songs that are very piano-based--that's why they probably searched `piano`.  Regressing the relevance score instead would allow better post-processing where maybe we can sort the search results from highest to lowest `relevance_score` so the user is happier with the search results.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "[1] Humphrey, Eric J., Durand, Simon, and McFee, Brian. \"OpenMIC-2018: An Open Dataset for Multiple Instrument Recognition.\" in Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), 2018.\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Music_information_retrieval#:~:text=Music%20information%20retrieval%20(MIR)%20is,with%20many%20real%2Dworld%20applications.\n",
    "\n",
    "[3] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron Weiss, & Kevin Wilson (2017). CNN Architectures for Large-Scale Audio Classification. In International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a905ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
