{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elilaird/CS83212-Advanced-Neural-Networks/blob/main/Lab2_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf5OYNnUIGrM"
   },
   "source": [
    "# Lab 2: Style Transfer\n",
    "\n",
    "**Group Members:**\n",
    "* Clay Harper\n",
    "* Eli Laird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quM4QpEhKSp0",
    "outputId": "ac58c40b-204e-40f0-fcc7-37b345826914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.4.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "#from tqdm import \n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, Model, Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D, Input, UpSampling2D, Conv2DTranspose, Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaQOhCt5J23c"
   },
   "source": [
    "## VGG Manipulation\n",
    "\n",
    "Here, we need to manipulate the given VGG code (courtesy of Justin Ledford) to make use of pooling layers or strided convolutions alternatively.  We chose to use strided convolutions because it is less computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le6MnUcMIAy2",
    "outputId": "85376331-57ea-49c2-92c8-8ef67f291e8f"
   },
   "outputs": [],
   "source": [
    "# Load VGG\n",
    "pre_trained_model = tf.keras.applications.VGG19(include_top=False,\n",
    "                                                      weights='imagenet')\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Block 1\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "        if target_layer == 1:\n",
    "            return x\n",
    "        # Strides instead of maxpooling \n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "        if target_layer == 2:\n",
    "            return x\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "        if target_layer == 3:\n",
    "            return x\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "        # Block 4\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "        if target_layer == 4:\n",
    "            return x\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "        # Block 5\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "        return x\n",
    "\n",
    "def load_weights(trained_model, model):\n",
    "    layer_names = [layer.name for layer in trained_model.layers]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            layer.set_weights(trained_model.get_layer(b_name).get_weights())\n",
    "            layer.trainable = False\n",
    "\n",
    "def VGG19(trained_model, input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    load_weights(trained_model, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjDwH6xSTBMW"
   },
   "source": [
    "Create an encoder network from the pretrained VGG network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhDJoX1bIGcL",
    "outputId": "b4ae4fd1-e532-46bc-f35a-af270b6abc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "=================================================================\n",
      "Total params: 555,328\n",
      "Trainable params: 0\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_layer = 3\n",
    "vgg_model = VGG19(pre_trained_model, input_shape=(256, 256, 3), target_layer=target_layer)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur4iav2GTkyH"
   },
   "source": [
    "## Decoder Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6aMf8aNhTUsc"
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    with tf.device('/cpu:0'): #******change to gpu later\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "        if layer == 5:\n",
    "            return x\n",
    "\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block4_2DTrans')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "        if layer == 4:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block3_2DTrans')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "        if layer == 3:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block2_2DTrans')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "        if layer == 2:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block1_2DTrans')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "        if layer == 1:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matching\n",
    "\n",
    "We decided to implement a covariance matching algorithm for the section where we got to choose one of the following:\n",
    "\n",
    "* smoothing constraint\n",
    "* style transfer with an image mask\n",
    "* different covariance matching algorithm\n",
    "* blend different styles in the reconstruction\n",
    "\n",
    "The implementation we are using was proposed in https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_A_Closed-Form_Solution_to_Universal_Style_Transfer_ICCV_2019_paper.pdf\n",
    "\n",
    "The closed form solution of the transformation matrix is:\n",
    "\n",
    "$$T = \\Sigma_c^{-1/2}(\\Sigma_c^{1/2}\\Sigma_s\\Sigma_c^{1/2})^{1/2}\\Sigma_c^{-1/2}$$\n",
    "\n",
    "Where taking a matrix to a power is defined as:\n",
    "\n",
    "$$A^\\alpha = \\Gamma\\Lambda^\\alpha\\Gamma^T$$\n",
    "\n",
    "Since the style image will always be the same in this architecture, we precompute the covariance of the style image and use it as a global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_s = np.cov(style_img)\n",
    "\n",
    "def matrix_to_power(matrix, power):\n",
    "    # Assuming real symmetric\n",
    "    e_val, e_vec = np.linalg.eigh(matrix)\n",
    "    lambda_ = np.diag(e_val**power)\n",
    "\n",
    "    return e_vec@lambda_@e_vec.T\n",
    "\n",
    "def new_transform(content_img, style_img):\n",
    "    sigma_c = np.cov(content_img)\n",
    "    sigma_s = np.cov(style_img)\n",
    "\n",
    "    simga_c_neg_half = matrix_to_power(sigma_c, -1/2)\n",
    "    sigma_c_half = matrix_to_power(sigma_c, 1/2)\n",
    "\n",
    "    inner = matrix_to_power(sigma_c_half@sigma_s@sigma_c_half, 1/2)\n",
    "\n",
    "    transform_mat = simga_c_neg_half@inner@simga_c_neg_half\n",
    "    return transform_mat@content_img\n",
    "\n",
    "class CovarianceMatching(Layer):\n",
    "    def call(self, xin)  :\n",
    "        xout = tf.py_function(new_transform, \n",
    "                           [xin],\n",
    "                           'float32',\n",
    "                           name='cov_matching')\n",
    "        xout = K.stop_gradient(xout) # explicitly set no grad\n",
    "        xout.set_shape(xin.shape) # explicitly set output shape\n",
    "        return xout\n",
    "    \n",
    "    def compute_output_shape(self, sin) :\n",
    "        return sin.shape\n",
    "\n",
    "# tf.py_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96q2OyoLUqIq"
   },
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J1zyMow9UmVL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "LAMBDA=1\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, trained_model, input_shape=(256, 256, 3), target_layer=5, style_path='../data/style.jpg', decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        self.trained_model = trained_model\n",
    "\n",
    "    \n",
    "\n",
    "        self.encoder = VGG19(self.trained_model, input_shape=self.input_shape, target_layer=target_layer)\n",
    "        \n",
    "        # Load style image and add batch dim\n",
    "        self.style_image = np.expand_dims(img_to_array(load_img(style_path, target_size=self.input_shape)),\n",
    "                                          axis=0)\n",
    "    \n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "            \n",
    "        # Create the complete model (encoder -> covar_match -> decoder)\n",
    "        transformed_tensor = CovarianceMatching(name='covar_matching')(self.encoder.output)\n",
    "        decoder_output = self.decoder(transformed_tensor)\n",
    "\n",
    "        self.model = Model(self.encoder.input, decoder_output)\n",
    "        \n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                  LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output.shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "    \n",
    "#     def create_transform_layer(self):\n",
    "#         inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "#         output = new_transform(inputs, tf.constant(self.sigma_style, dtype=tf.float32))\n",
    "#         return Model(inputs, output)\n",
    "    \n",
    "#     def load_style_img(self, img_path):\n",
    "#         img = image.load_img(img_path)\n",
    "#         img = image.img_to_array(img)\n",
    "#         img = resize(img, (self.input_shape[0], self.input_shape[1]))\n",
    "#         #img = tf.reshape(img, [self.input_shape[2], self.input_shape[0], self.input_shape[1]])\n",
    "#         return tf.expand_dims(img, axis=0)\n",
    "    \n",
    "    def process_style_img(self, img):\n",
    "        out = self.encoder(img)  \n",
    "        out = tf.squeeze(out)\n",
    "        out = tf.reshape(out, [out.shape[2], out.shape[0] * out.shape[1]])\n",
    "#         return \n",
    "#         return tf_cov(out)\n",
    "    \n",
    "    def resize_for_transform(self,img):\n",
    "        img = tf.squeeze(img)\n",
    "\n",
    "        img = tf.reshape(img, [self.input_shape[2], self.input_shape[0], self.input_shape[1]])\n",
    "        return img\n",
    "    \n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFOPUQs9WM89"
   },
   "source": [
    "## Train Two Decoders \n",
    "\n",
    "Decoders will be created based on the outputs of 2 different layers in the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFCt6eYuVEWS",
    "outputId": "c8b7765e-f9c9-4585-a9ba-c678256aefca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "covar_matching (CovarianceMa (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "decoder_3 (Functional)       (None, 256, 256, 3)       10060805  \n",
      "=================================================================\n",
      "Total params: 10,616,133\n",
      "Trainable params: 10,060,805\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder = EncoderDecoder(pre_trained_model, target_layer=target_layer)\n",
    "encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.45 GiB (download: 1.45 GiB, generated: 1.46 GiB, total: 2.91 GiB) to /Users/clayharper/tensorflow_datasets/imagenette/full-size-v2/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627a8452d3be403a9022f8d3b30cf648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d7cacc49904037b00c79bf528b0977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5243948664ee45e89dafae536145fa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ae82da917dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a dataset to train on (imagenette)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imagenette\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    444\u001b[0m             self._download_and_prepare(\n\u001b[1;32m    445\u001b[0m                 \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m             )\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0;32m-> 1148\u001b[0;31m           \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptional_pipeline_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m       )\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/image_classification/imagenette.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_URL_PREFIX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{}.tgz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mval_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/promise/promise.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/promise/promise.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# type: (Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/promise/promise.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (Promise, Optional[float]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0masync_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/promise/async_.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# fulfilled or rejected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdrain_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/site-packages/promise/schedulers/immediate.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpromise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_then\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_resolve_or_reject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mwaited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwaited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/clayharper/anaconda/envs/ml_spring_21/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a dataset to train on (imagenette)\n",
    "data_loader = tfds.load(\"imagenette\", as_supervised=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Pp2COm9zVIml",
    "outputId": "da93c3c8-c68a-4d09-97fd-c0c06c9d4ab9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Eli's Disk/datasets/imagenette2/train\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "model_26 (Functional)        (1, 64, 64, 256)          0         \n",
      "_________________________________________________________________\n",
      "decoder_3 (Functional)       (None, 256, 256, 3)       10060805  \n",
      "=================================================================\n",
      "Total params: 10,616,133\n",
      "Trainable params: 10,060,805\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "#from scipy.misc import imresize, imsave DEPRACATED\n",
    "#from cv2 import resize\n",
    "from tensorflow.image import resize\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "#from model import EncoderDecoder\n",
    "#from util import count_num_samples\n",
    "\n",
    "DATASET_DIR = '/Volumes/Eli\\'s Disk/datasets'\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, 'imagenette2', 'train')\n",
    "VAL_PATH = os.path.join(DATASET_DIR, 'imagenette2', 'val')\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 1\n",
    "epochs = 2\n",
    "\n",
    "print(TRAIN_PATH)\n",
    "datagen = ImageDataGenerator()\n",
    "'''train_gen = datagen.flow_from_directory(TRAIN_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(VAL_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)'''\n",
    "\n",
    "(train,val), info = tfds.load(name='imagenette', \n",
    "                       batch_size=BATCH_SIZE, \n",
    "                       split=['train','validation'], \n",
    "                       with_info=True,\n",
    "                       download=True)\n",
    "\n",
    "\n",
    "def create_gen(ds):\n",
    "    def tuple_gen():\n",
    "        for i in ds:\n",
    "\n",
    "            # (X, y)\n",
    "            \n",
    "            img = tf.keras.applications.vgg19.preprocess_input(i['image'])\n",
    "            img = tf.image.resize(img, TARGET_SIZE)\n",
    "            yield (img,img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "train_gen = create_gen(train)\n",
    "val_gen = create_gen(val)\n",
    "'''train_gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "val_gen = create_gen(VAL_PATH, TARGET_SIZE, BATCH_SIZE)'''\n",
    "\n",
    "# This needs to be in scope where model is defined\n",
    "class OutputPreview(Callback):\n",
    "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = resize(test_target, (256,256)) #imresize(test_img, (256, 256, 3))\n",
    "        #test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            imsave(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "\n",
    "num_samples = 9469 #count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "target_layer = 3 #int(sys.argv[1])\n",
    "\n",
    "encoder_decoder = EncoderDecoder(trained_model=pre_trained_model, target_layer=target_layer)\n",
    "encoder_decoder.summary()\n",
    "\n",
    "callbacks = [OutputPreview(encoder_decoder, '../data/doge.jpg', 5000, './preview-%d' % target_layer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "I0QDOgptL6jG",
    "outputId": "2752fb7e-7108-4fe8-819c-da5b5480dad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-10-e471bb66f049>:40 loss  *\n        encoding_in = get_encodings(img_in)\n    <ipython-input-10-e471bb66f049>:36 get_encodings  *\n        encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n    <ipython-input-51-e2ef8083919b>:61 VGG19  *\n        model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n    <ipython-input-51-e2ef8083919b>:8 vgg_layers  *\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:952 __call__  **\n        input_list)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1091 _functional_construction_call\n        inputs, input_masks, args, kwargs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:822 _keras_tensor_symbolic_call\n        return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:862 _infer_output_signature\n        self._maybe_build(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-328284d77a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_decoder.model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n\u001b[0;32m----> 2\u001b[0;31m         epochs=epochs, callbacks=callbacks, validation_data=val_gen)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-10-e471bb66f049>:40 loss  *\n        encoding_in = get_encodings(img_in)\n    <ipython-input-10-e471bb66f049>:36 get_encodings  *\n        encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n    <ipython-input-51-e2ef8083919b>:61 VGG19  *\n        model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n    <ipython-input-51-e2ef8083919b>:8 vgg_layers  *\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:952 __call__  **\n        input_list)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1091 _functional_construction_call\n        inputs, input_masks, args, kwargs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:822 _keras_tensor_symbolic_call\n        return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:862 _infer_output_signature\n        self._maybe_build(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder.model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, callbacks=callbacks, validation_data=val_gen)\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXVOqTU7kbsl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jarB6zyyjr1Q"
   },
   "source": [
    "# Covariance matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7vdqwrnlR_B"
   },
   "source": [
    "## Dr. Larson \n",
    "\n",
    "Reusing code to test if mine works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "bSM1DMULjtSR",
    "outputId": "29516c6e-162a-4d35-b178-b7b6278403d3"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "channels = 5 # number of channels to simulate\n",
    "hw = 256 # height and width dimensions\n",
    "\n",
    "# Create some Activation tensors\n",
    "A1 = np.random.randn(channels,hw,hw)\n",
    "A2 = np.random.rand(channels,hw,hw)\n",
    "\n",
    "# make A1 low frequency  \n",
    "filt = np.ones((3,6,6))\n",
    "filt = filt / np.sum(filt)\n",
    "A1 = convolve(A1,filt)\n",
    "A1 = A1 - np.mean(A1)\n",
    "\n",
    "# make A2 high frequency\n",
    "filt = np.ones((2,3,3))\n",
    "filt = filt / np.sum(filt)\n",
    "filt[:,:,0] = filt[:,:,0]*(-1)\n",
    "filt[:,:,1] = 0\n",
    "A2 = convolve(A2,filt)\n",
    "A2 = A2 - np.mean(A2)\n",
    "\n",
    "# plot each tensor, average along channel\n",
    "def show_active(A,title=''):\n",
    "    plt.imshow(np.mean(A,0))\n",
    "    plt.title(title)\n",
    "\n",
    "n_plots = min(channels,10)\n",
    "plt.figure(figsize=(15,5))   \n",
    "for i in range(n_plots):\n",
    "    plt.subplot(1,n_plots,i+1)\n",
    "    plt.imshow(A1[i,:,:])\n",
    "    plt.title(f'A1 at {i}')\n",
    "    \n",
    "n_plots = min(channels,10)\n",
    "plt.figure(figsize=(15,5))   \n",
    "for i in range(n_plots):\n",
    "    plt.subplot(1,n_plots,i+1)\n",
    "    plt.imshow(A2[i,:,:])\n",
    "    plt.title(f'A2 at {i}')\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "show_active(A1,'A1 Mean over channels')\n",
    "plt.subplot(1,2,2)\n",
    "show_active(A2,'A2 Mean over channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "w9VvsCBKlWYO",
    "outputId": "415a1c24-a7c5-47d1-f12e-3588c849b31f"
   },
   "outputs": [],
   "source": [
    "# calculate gram matrix (vectorized spatially) and show it\n",
    "# for each activations\n",
    "def gram(x):\n",
    "    tmp = x.reshape((channels,hw*hw))\n",
    "    return tmp @ tmp.T / (channels*hw*hw)\n",
    "\n",
    "def show_gram(A,title=''):\n",
    "    plt.figure()\n",
    "    G = gram(A)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(G)\n",
    "    plt.title('Gram of '+title)\n",
    "    plt.subplot(1,2,1)\n",
    "    show_active(A,title)\n",
    "    return G\n",
    "    \n",
    "\n",
    "G_A1 = show_gram(A1,'A1')\n",
    "G_A2 = show_gram(A2,'A2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZwKUaS3IlZIa",
    "outputId": "2e9f6869-3d64-4a9f-c6cd-24db0477a581"
   },
   "outputs": [],
   "source": [
    "def whiten_cov(A):\n",
    "    Avec = A.reshape((channels,hw*hw))\n",
    "    AA = Avec @ Avec.T\n",
    "    # notice that the SVD is of the covariance matrix\n",
    "    u,s,vt = np.linalg.svd(AA,full_matrices=False)\n",
    "    \n",
    "    A_white = ((u @ np.diag(s**(-0.5)) @ vt) @ Avec) # no usage of the singular values here\n",
    "    return A_white, (u, s, vt), Avec\n",
    "    \n",
    "\n",
    "A1_white, A1SVDcov, A1vec = whiten_cov(A1)\n",
    "A2_white, A2SVDcov, A2vec = whiten_cov(A2)\n",
    "    \n",
    "u1, s1, _ = A1SVDcov\n",
    "u2, s2, _ = A2SVDcov\n",
    "\n",
    "# -------A1-------\n",
    "# Apply inverse reconstruction to get colored version\n",
    "A1_mixed_color = u2 @ np.diag(s2**(0.5)) @ u2.T @ A1_white\n",
    "A1_mixed_color = A1_mixed_color.reshape((channels,hw,hw))\n",
    "\n",
    "# -------A2-------\n",
    "# Apply inverse reconstruction to get colored version\n",
    "A2_mixed_color = u1 @ np.diag(s1**(0.5)) @ u1.T @ A2_white\n",
    "A2_mixed_color = A2_mixed_color.reshape((channels,hw,hw))\n",
    "\n",
    "diffA1 = np.mean((A1_mixed_color-A1)**2)\n",
    "diffA2 = np.mean((A2_mixed_color-A2)**2)\n",
    "\n",
    "# show originals\n",
    "# and colored versions\n",
    "G_A1 = show_gram(A1,'A1')\n",
    "_ = show_gram(A1_white.reshape((channels,hw,hw)),'A1_white')\n",
    "G_A1asA2 = show_gram(A1_mixed_color,'A1_mixed_color')\n",
    "\n",
    "# show originals\n",
    "# and colored versions\n",
    "G_A1 = show_gram(A2,'A2')\n",
    "_ = show_gram(A2_white.reshape((channels,hw,hw)),'A2_white')\n",
    "G_A2asA1 = show_gram(A2_mixed_color,'A2_mixed_color')\n",
    "\n",
    "print('A1 average content difference:', diffA1)\n",
    "print('A2 average content difference:', diffA2)\n",
    "\n",
    "print('A1 colored average Grammian difference:', np.mean((G_A2-G_A1asA2)**2))\n",
    "print('A2 colored average Grammian difference:', np.mean((G_A1-G_A2asA1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAvFBYsXlssm"
   },
   "source": [
    "## New implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtyjBIeImmiu"
   },
   "source": [
    "Using the closed form solution of the transformation matrix is:\n",
    "\n",
    "$$T = \\Sigma_c^{-1/2}(\\Sigma_c^{1/2}\\Sigma_s\\Sigma_c^{1/2})^{1/2}\\Sigma_c^{-1/2}$$\n",
    "\n",
    "Useful link: https://stats.stackexchange.com/questions/363425/raising-a-variance-covariance-matrix-to-a-negative-half-power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1pnvsqLlcrO"
   },
   "outputs": [],
   "source": [
    "def matrix_to_power(matrix, power):\n",
    "    # Assuming real symmetric\n",
    "    e_val, e_vec = np.linalg.eigh(matrix)\n",
    "    lambda_ = np.diag(e_val**power)\n",
    "\n",
    "    return e_vec@lambda_@e_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r10Xptjzhit"
   },
   "outputs": [],
   "source": [
    "def new_transform(content_img, style_img):\n",
    "    sigma_c = np.cov(content_img)\n",
    "    sigma_s = np.cov(style_img)\n",
    "\n",
    "    simga_c_neg_half = matrix_to_power(sigma_c, -1/2)\n",
    "    sigma_c_half = matrix_to_power(sigma_c, 1/2)\n",
    "\n",
    "    inner = matrix_to_power(sigma_c_half@sigma_s@sigma_c_half, 1/2)\n",
    "\n",
    "    transform_mat = simga_c_neg_half@inner@simga_c_neg_half\n",
    "    return transform_mat@content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = A1.reshape(channels, hw*hw)\n",
    "style = A2.reshape(channels, hw*hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored = new_transform(content, style)\n",
    "np.count_nonzero(np.isnan(colored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_gram(colored.reshape((channels,hw,hw)),'A1 with A2 style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lab2-Style Transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
