{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elilaird/CS83212-Advanced-Neural-Networks/blob/main/Lab2_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf5OYNnUIGrM"
   },
   "source": [
    "# Lab 2: Style Transfer\n",
    "\n",
    "**Group Members:**\n",
    "* Clay Harper\n",
    "* Eli Laird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quM4QpEhKSp0",
    "outputId": "ac58c40b-204e-40f0-fcc7-37b345826914"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, Model, Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D, Input, UpSampling2D, Conv2DTranspose, Layer\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaQOhCt5J23c"
   },
   "source": [
    "## VGG Manipulation\n",
    "\n",
    "Here, we need to manipulate the given VGG code (courtesy of Justin Ledford) to make use of pooling layers or strided convolutions alternatively.  We chose to use strided convolutions because it is less computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le6MnUcMIAy2",
    "outputId": "85376331-57ea-49c2-92c8-8ef67f291e8f"
   },
   "outputs": [],
   "source": [
    "# Load VGG\n",
    "pre_trained_model = tf.keras.applications.VGG19(include_top=False,\n",
    "                                                      weights='imagenet')\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    # Strides instead of maxpooling \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', strides=2)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', strides=2)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4', strides=2)(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4', strides=2)(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "def load_weights(trained_model, model):\n",
    "    layer_names = [layer.name for layer in trained_model.layers]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            layer.set_weights(trained_model.get_layer(b_name).get_weights())\n",
    "            layer.trainable = False\n",
    "\n",
    "def VGG19(trained_model, input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    load_weights(trained_model, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjDwH6xSTBMW"
   },
   "source": [
    "Create an encoder network from the pretrained VGG network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhDJoX1bIGcL",
    "outputId": "b4ae4fd1-e532-46bc-f35a-af270b6abc49"
   },
   "outputs": [],
   "source": [
    "target_layer = 3\n",
    "vgg_model = VGG19(pre_trained_model, input_shape=(256, 256, 3), target_layer=target_layer)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur4iav2GTkyH"
   },
   "source": [
    "## Decoder Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aMf8aNhTUsc"
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 5:\n",
    "        return x\n",
    "\n",
    "\n",
    "    #x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block4_2DTrans')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return x\n",
    "\n",
    "    #x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block3_2DTrans')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    #x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block2_2DTrans')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    #x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block1_2DTrans')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 1:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96q2OyoLUqIq"
   },
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1zyMow9UmVL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "LAMBDA=1\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, trained_model, input_shape=(256, 256, 3), target_layer=5, decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        self.trained_model = trained_model\n",
    "\n",
    "        self.encoder = VGG19(self.trained_model, input_shape=self.input_shape, target_layer=target_layer)\n",
    "\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "   \n",
    "        decoder_output = self.decoder(self.encoder.output)\n",
    "\n",
    "        self.model = Model(self.encoder.input, decoder_output)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output.shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        decoder = Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "        return decoder\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFOPUQs9WM89"
   },
   "source": [
    "## Train Two Decoders \n",
    "\n",
    "Decoders will be created based on the outputs of 2 different layers in the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFCt6eYuVEWS",
    "outputId": "c8b7765e-f9c9-4585-a9ba-c678256aefca",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(pre_trained_model, target_layer=target_layer)\n",
    "encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset to train on (imagenette)\n",
    "data_loader = tfds.load(\"imagenette\", download=True)\n",
    "train_ds, test_ds = data_loader['train'], data_loader['validation']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize\n",
    "train_ds, test_ds = data_loader['train'], data_loader['validation']\n",
    "\n",
    "# Add batches and preprocessing \n",
    "BATCH_SIZE=1\n",
    "n_train_observations = train_ds.cardinality().numpy()\n",
    "n_test_observations = test_ds.cardinality().numpy()\n",
    "\n",
    "steps_per_epoch = n_train_observations//BATCH_SIZE + n_train_observations%BATCH_SIZE\n",
    "validation_steps = n_test_observations//BATCH_SIZE + n_test_observations%BATCH_SIZE\n",
    "\n",
    "def preprocess(observation):\n",
    "    img = observation['image']\n",
    "    \n",
    "    # Resize to target shape\n",
    "    processed_img = resize(img, encoder_decoder.input_shape[:2])\n",
    "    \n",
    "    # vgg preprocess\n",
    "    processed_img = vgg_preprocess(processed_img)\n",
    "    \n",
    "    # get the vgg encoding for the 'label'\n",
    "    encoded_img = encoder_decoder.encoder(tf.expand_dims(processed_img, axis=0))\n",
    "    encoded_img = tf.squeeze(encoded_img)\n",
    "\n",
    "    return processed_img, processed_img #for encoder/decoder loss\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    lambda image: preprocess(image)).shuffle(1000).batch(BATCH_SIZE).repeat()\n",
    "test_ds = test_ds.map(\n",
    "    lambda image: preprocess(image)).shuffle(1000).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "\n",
    "print(f'Train observations: {n_train_observations}')\n",
    "print(f'Test observations: {n_test_observations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class CustLoss(Loss):\n",
    "    def __init__(self, encoder, target_layer, LAMBDA=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.target_layer = target_layer\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def get_encodings(self, inputs):\n",
    "        return self.encoder(inputs)\n",
    "    \n",
    "    def l2_loss(self, x):\n",
    "        return K.sum(K.square(x)) / 2\n",
    "        \n",
    "    def call(self, img_in, img_out):\n",
    "        encoding_in = self.get_encodings(img_in)\n",
    "        encoding_out = self.get_encodings(img_out)\n",
    "        return self.l2_loss(img_out - img_in) + self.LAMBDA*self.l2_loss(encoding_out-encoding_in)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'encoder': self.encoder,\n",
    "                'target_layer': self.target_layer, 'LAMBDA': self.LAMBDA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(1e-4)\n",
    "model = encoder_decoder.model\n",
    "encoder = encoder_decoder.encoder\n",
    "model.compile(loss=CustLoss(encoder, target_layer), optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath='./encoder_decoder.h5'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=.1, patience=5, min_lr=1e-6),\n",
    "    EarlyStopping(patience=7)\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, validation_data=test_ds,\n",
    "                    epochs=1000, callbacks=callbacks,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lab2-Style Transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
