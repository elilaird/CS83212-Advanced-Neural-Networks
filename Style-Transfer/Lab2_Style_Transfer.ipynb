{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elilaird/CS83212-Advanced-Neural-Networks/blob/main/Lab2_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf5OYNnUIGrM"
   },
   "source": [
    "# Lab 2: Style Transfer\n",
    "\n",
    "**Group Members:**\n",
    "* Clay Harper\n",
    "* Eli Laird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quM4QpEhKSp0",
    "outputId": "ac58c40b-204e-40f0-fcc7-37b345826914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.4.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "#from tqdm import \n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, Model, Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D, Input, UpSampling2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img\n",
    "\n",
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title==None:\n",
    "        title = str(image.shape)\n",
    "    else:\n",
    "        title += ' '+str(image.shape)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy=\n",
       "array([[[[0.19616653, 0.2370979 , 0.01382698],\n",
       "         [0.3255247 , 0.33125383, 0.04577206],\n",
       "         [0.23174787, 0.259532  , 0.00837929],\n",
       "         ...,\n",
       "         [0.8826823 , 0.2768057 , 0.02008655],\n",
       "         [0.8455558 , 0.2832089 , 0.02418428],\n",
       "         [0.67999774, 0.22419004, 0.06713389]],\n",
       "\n",
       "        [[0.31433633, 0.27813652, 0.03707491],\n",
       "         [0.2868413 , 0.28530943, 0.00491154],\n",
       "         [0.18854359, 0.26385188, 0.02077015],\n",
       "         ...,\n",
       "         [0.9501513 , 0.30647022, 0.01543352],\n",
       "         [0.78165984, 0.26972082, 0.10465878],\n",
       "         [0.35265398, 0.07610869, 0.08463734]],\n",
       "\n",
       "        [[0.43762833, 0.35797334, 0.04382085],\n",
       "         [0.3147691 , 0.27480277, 0.00835057],\n",
       "         [0.20288375, 0.28504902, 0.01923637],\n",
       "         ...,\n",
       "         [0.92785126, 0.33756128, 0.10213695],\n",
       "         [0.80778575, 0.34528953, 0.30413797],\n",
       "         [0.448022  , 0.17248392, 0.25845972]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.36846665, 0.4783318 , 0.3499713 ],\n",
       "         [0.43346167, 0.5495462 , 0.36490697],\n",
       "         [0.5208161 , 0.5967754 , 0.37368453],\n",
       "         ...,\n",
       "         [0.00264055, 0.01184896, 0.01727175],\n",
       "         [0.00459559, 0.01023284, 0.01746324],\n",
       "         [0.00459559, 0.01023284, 0.01746324]],\n",
       "\n",
       "        [[0.45078704, 0.44186395, 0.23637217],\n",
       "         [0.4619658 , 0.45286462, 0.22537151],\n",
       "         [0.47494638, 0.45349073, 0.22249734],\n",
       "         ...,\n",
       "         [0.0018076 , 0.00181717, 0.0060049 ],\n",
       "         [0.00385072, 0.00201248, 0.00752719],\n",
       "         [0.00582108, 0.00398284, 0.00949755]],\n",
       "\n",
       "        [[0.41279876, 0.27774972, 0.0832644 ],\n",
       "         [0.41974384, 0.28738895, 0.07706036],\n",
       "         [0.41722584, 0.26306105, 0.0715782 ],\n",
       "         ...,\n",
       "         [0.00814951, 0.00876226, 0.00437921],\n",
       "         [0.00915863, 0.00915863, 0.00915863],\n",
       "         [0.01354167, 0.01354167, 0.01354167]]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.image import resize\n",
    "\n",
    "style_img = load_img('../data/style.jpg')\n",
    "\n",
    "style_img = resize(style_img, (256,256)) \n",
    "\n",
    "#style_arr = image.img_to_array(tf.squeeze(style_img))\n",
    "#test_target = np.expand_dims(test_target, axis=0)'''\n",
    "\n",
    "tensor_to_image(style_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 256, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "style_img = image.load_img('../data/style.jpg')\n",
    "test = image.img_to_array(style_img)\n",
    "test = resize(test, (256,256)) #imresize(test_img, (256, 256, 3))\n",
    "tf.reshape(test, [3,256,256]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaQOhCt5J23c"
   },
   "source": [
    "## VGG Manipulation\n",
    "\n",
    "Here, we need to manipulate the given VGG code (courtesy of Justin Ledford) to make use of pooling layers or strided convolutions alternatively.  We chose to use strided convolutions because it is less computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le6MnUcMIAy2",
    "outputId": "85376331-57ea-49c2-92c8-8ef67f291e8f"
   },
   "outputs": [],
   "source": [
    "# Load VGG\n",
    "pre_trained_model = tf.keras.applications.VGG19(include_top=False,\n",
    "                                                      weights='imagenet')\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Block 1\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "        if target_layer == 1:\n",
    "            return x\n",
    "        # Strides instead of maxpooling \n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "        if target_layer == 2:\n",
    "            return x\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "        if target_layer == 3:\n",
    "            return x\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "        # Block 4\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "        if target_layer == 4:\n",
    "            return x\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4', strides=2)(x)\n",
    "        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "        # Block 5\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "        return x\n",
    "\n",
    "def load_weights(trained_model, model):\n",
    "    layer_names = [layer.name for layer in trained_model.layers]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            layer.set_weights(trained_model.get_layer(b_name).get_weights())\n",
    "            layer.trainable = False\n",
    "\n",
    "def VGG19(trained_model, input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    load_weights(trained_model, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjDwH6xSTBMW"
   },
   "source": [
    "Create an encoder network from the pretrained VGG network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhDJoX1bIGcL",
    "outputId": "b4ae4fd1-e532-46bc-f35a-af270b6abc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "=================================================================\n",
      "Total params: 555,328\n",
      "Trainable params: 0\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_layer = 3\n",
    "vgg_model = VGG19(pre_trained_model, input_shape=(256, 256, 3), target_layer=target_layer)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur4iav2GTkyH"
   },
   "source": [
    "## Decoder Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6aMf8aNhTUsc"
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    with tf.device('/GPU:0'):\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "        if layer == 5:\n",
    "            return x\n",
    "\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block4_2DTrans')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "        if layer == 4:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block3_2DTrans')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "        if layer == 3:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block2_2DTrans')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "        if layer == 2:\n",
    "            return x\n",
    "\n",
    "        #x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "        x = Conv2DTranspose(1, kernel_size=(4,4), padding='same', strides=(2,2), name='decoder_block1_2DTrans')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "        if layer == 1:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://stackoverflow.com/questions/47709854/how-to-get-covariance-matrix-in-tensorflow\n",
    "def tf_cov(x):\n",
    "    with tf.device('/CPU:0'):\n",
    "        x = x - tf.expand_dims(tf.reduce_mean(x, axis=1), 1)\n",
    "        fact = tf.cast(tf.shape(x)[1] - 1, tf.float32)\n",
    "        b = tf.cast(tf.math.conj(tf.transpose(x)), tf.float32)\n",
    "        return tf.matmul(x, b) / fact\n",
    "\n",
    "\n",
    "def matrix_to_power(matrix, power):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # Assuming real symmetric\n",
    "        e_val, e_vec = tf.linalg.eigh(matrix)\n",
    "        lambda_ = tf.linalg.diag(e_val**power)\n",
    "\n",
    "        return e_vec@lambda_@tf.transpose(e_vec)\n",
    "\n",
    "def new_transform(content_img, sigma_s):\n",
    "    with tf.device('/CPU:0'):\n",
    "        h,w,c = content_img.shape[1:]\n",
    "        content_img = tf.squeeze(content_img)\n",
    "        content_img = tf.reshape(content_img, [h,w,c])\n",
    "        content_img = tf.reshape(content_img, [c, h * w]) \n",
    "\n",
    "        sigma_c = tf_cov(content_img)\n",
    "\n",
    "        sigma_c_neg_half = matrix_to_power(sigma_c, -1/2)\n",
    "        sigma_c_half = matrix_to_power(sigma_c, 1/2)\n",
    "        inner = matrix_to_power(sigma_c_half@sigma_s@sigma_c_half, 1/2)\n",
    "        transform_mat = sigma_c_neg_half@inner@sigma_c_neg_half\n",
    "\n",
    "        out = transform_mat@content_img\n",
    "        out = tf.reshape(out, [c,h,w])\n",
    "        out = tf.expand_dims(tf.reshape(out, [h,w,c]),0)\n",
    "      \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96q2OyoLUqIq"
   },
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J1zyMow9UmVL"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "LAMBDA=1\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, trained_model, input_shape=(256, 256, 3), target_layer=5, style_path='../data/style.jpg', decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "        self.trained_model = trained_model\n",
    "\n",
    "    \n",
    "\n",
    "        self.encoder = VGG19(self.trained_model, input_shape=self.input_shape, target_layer=target_layer)\n",
    "        \n",
    "        self.style_image = self.load_style_img(style_path)\n",
    "        self.sigma_style = self.process_style_img(self.style_image)\n",
    "        \n",
    "        self.transform = self.create_transform_layer()\n",
    "    \n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "\n",
    "        self.model = Model(self.encoder.input, self.decoder(self.transform(self.encoder.output)))\n",
    "        \n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                  LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.transform.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "    \n",
    "    def create_transform_layer(self):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        output = new_transform(inputs, tf.constant(self.sigma_style, dtype=tf.float32))\n",
    "        return Model(inputs, output)\n",
    "    \n",
    "    def load_style_img(self, img_path):\n",
    "        img = image.load_img(img_path)\n",
    "        img = image.img_to_array(img)\n",
    "        img = resize(img, (self.input_shape[0], self.input_shape[1]))\n",
    "        #img = tf.reshape(img, [self.input_shape[2], self.input_shape[0], self.input_shape[1]])\n",
    "        return tf.expand_dims(img, axis=0)\n",
    "    \n",
    "    def process_style_img(self, img):\n",
    "        out = self.encoder(img)  \n",
    "        out = tf.squeeze(out)\n",
    "        out = tf.reshape(out, [out.shape[2], out.shape[0] * out.shape[1]]) \n",
    "        return tf_cov(out)\n",
    "    \n",
    "    def resize_for_transform(self,img):\n",
    "        img = tf.squeeze(img)\n",
    "\n",
    "        img = tf.reshape(img, [self.input_shape[2], self.input_shape[0], self.input_shape[1]])\n",
    "        return img\n",
    "    \n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFOPUQs9WM89"
   },
   "source": [
    "## Train Two Decoders \n",
    "\n",
    "Decoders will be created based on the outputs of 2 different layers in the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFCt6eYuVEWS",
    "outputId": "c8b7765e-f9c9-4585-a9ba-c678256aefca",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "model (Functional)           (1, 64, 64, 256)          0         \n",
      "_________________________________________________________________\n",
      "decoder_3 (Functional)       (None, 256, 256, 3)       10060805  \n",
      "=================================================================\n",
      "Total params: 10,616,133\n",
      "Trainable params: 10,616,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder = EncoderDecoder(pre_trained_model, target_layer=target_layer)\n",
    "encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0+nightly'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Pp2COm9zVIml",
    "outputId": "da93c3c8-c68a-4d09-97fd-c0c06c9d4ab9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Eli's Disk/datasets/imagenette2/train\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "model_26 (Functional)        (1, 64, 64, 256)          0         \n",
      "_________________________________________________________________\n",
      "decoder_3 (Functional)       (None, 256, 256, 3)       10060805  \n",
      "=================================================================\n",
      "Total params: 10,616,133\n",
      "Trainable params: 10,060,805\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "#from scipy.misc import imresize, imsave DEPRACATED\n",
    "#from cv2 import resize\n",
    "from tensorflow.image import resize\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "#from model import EncoderDecoder\n",
    "#from util import count_num_samples\n",
    "\n",
    "DATASET_DIR = '/Volumes/Eli\\'s Disk/datasets'\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, 'imagenette2', 'train')\n",
    "VAL_PATH = os.path.join(DATASET_DIR, 'imagenette2', 'val')\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 1\n",
    "epochs = 2\n",
    "\n",
    "print(TRAIN_PATH)\n",
    "datagen = ImageDataGenerator()\n",
    "'''train_gen = datagen.flow_from_directory(TRAIN_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(VAL_PATH, target_size=TARGET_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, class_mode=None)'''\n",
    "\n",
    "(train,val), info = tfds.load(name='imagenette', \n",
    "                       batch_size=BATCH_SIZE, \n",
    "                       split=['train','validation'], \n",
    "                       with_info=True,\n",
    "                       download=True)\n",
    "\n",
    "\n",
    "def create_gen(ds):\n",
    "    def tuple_gen():\n",
    "        for i in ds:\n",
    "\n",
    "            # (X, y)\n",
    "            \n",
    "            img = tf.keras.applications.vgg19.preprocess_input(i['image'])\n",
    "            img = tf.image.resize(img, TARGET_SIZE)\n",
    "            yield (img,img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "train_gen = create_gen(train)\n",
    "val_gen = create_gen(val)\n",
    "'''train_gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "val_gen = create_gen(VAL_PATH, TARGET_SIZE, BATCH_SIZE)'''\n",
    "\n",
    "# This needs to be in scope where model is defined\n",
    "class OutputPreview(Callback):\n",
    "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = resize(test_target, (256,256)) #imresize(test_img, (256, 256, 3))\n",
    "        #test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            imsave(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "\n",
    "num_samples = 9469 #count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "\n",
    "target_layer = 3 #int(sys.argv[1])\n",
    "\n",
    "encoder_decoder = EncoderDecoder(trained_model=pre_trained_model, target_layer=target_layer)\n",
    "encoder_decoder.summary()\n",
    "\n",
    "callbacks = [OutputPreview(encoder_decoder, '../data/doge.jpg', 5000, './preview-%d' % target_layer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "I0QDOgptL6jG",
    "outputId": "2752fb7e-7108-4fe8-819c-da5b5480dad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-10-e471bb66f049>:40 loss  *\n        encoding_in = get_encodings(img_in)\n    <ipython-input-10-e471bb66f049>:36 get_encodings  *\n        encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n    <ipython-input-51-e2ef8083919b>:61 VGG19  *\n        model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n    <ipython-input-51-e2ef8083919b>:8 vgg_layers  *\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:952 __call__  **\n        input_list)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1091 _functional_construction_call\n        inputs, input_masks, args, kwargs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:822 _keras_tensor_symbolic_call\n        return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:862 _infer_output_signature\n        self._maybe_build(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-328284d77a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_decoder.model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n\u001b[0;32m----> 2\u001b[0;31m         epochs=epochs, callbacks=callbacks, validation_data=val_gen)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-10-e471bb66f049>:40 loss  *\n        encoding_in = get_encodings(img_in)\n    <ipython-input-10-e471bb66f049>:36 get_encodings  *\n        encoder = VGG19(self.trained_model, inputs, self.input_shape, self.target_layer)\n    <ipython-input-51-e2ef8083919b>:61 VGG19  *\n        model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19', trainable=False)\n    <ipython-input-51-e2ef8083919b>:8 vgg_layers  *\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:952 __call__  **\n        input_list)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1091 _functional_construction_call\n        inputs, input_masks, args, kwargs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:822 _keras_tensor_symbolic_call\n        return self._infer_output_signature(inputs, args, kwargs, input_masks)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:862 _infer_output_signature\n        self._maybe_build(inputs)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:188 build\n        input_channel = self._get_input_channel(input_shape)\n    /Users/eli/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py:360 _get_input_channel\n        raise ValueError('The channel dimension of the inputs '\n\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder.model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, callbacks=callbacks, validation_data=val_gen)\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXVOqTU7kbsl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jarB6zyyjr1Q"
   },
   "source": [
    "# Covariance matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7vdqwrnlR_B"
   },
   "source": [
    "## Dr. Larson \n",
    "\n",
    "Reusing code to test if mine works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "bSM1DMULjtSR",
    "outputId": "29516c6e-162a-4d35-b178-b7b6278403d3"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "channels = 5 # number of channels to simulate\n",
    "hw = 256 # height and width dimensions\n",
    "\n",
    "# Create some Activation tensors\n",
    "A1 = np.random.randn(channels,hw,hw)\n",
    "A2 = np.random.rand(channels,hw,hw)\n",
    "\n",
    "# make A1 low frequency  \n",
    "filt = np.ones((3,6,6))\n",
    "filt = filt / np.sum(filt)\n",
    "A1 = convolve(A1,filt)\n",
    "A1 = A1 - np.mean(A1)\n",
    "\n",
    "# make A2 high frequency\n",
    "filt = np.ones((2,3,3))\n",
    "filt = filt / np.sum(filt)\n",
    "filt[:,:,0] = filt[:,:,0]*(-1)\n",
    "filt[:,:,1] = 0\n",
    "A2 = convolve(A2,filt)\n",
    "A2 = A2 - np.mean(A2)\n",
    "\n",
    "# plot each tensor, average along channel\n",
    "def show_active(A,title=''):\n",
    "    plt.imshow(np.mean(A,0))\n",
    "    plt.title(title)\n",
    "\n",
    "n_plots = min(channels,10)\n",
    "plt.figure(figsize=(15,5))   \n",
    "for i in range(n_plots):\n",
    "    plt.subplot(1,n_plots,i+1)\n",
    "    plt.imshow(A1[i,:,:])\n",
    "    plt.title(f'A1 at {i}')\n",
    "    \n",
    "n_plots = min(channels,10)\n",
    "plt.figure(figsize=(15,5))   \n",
    "for i in range(n_plots):\n",
    "    plt.subplot(1,n_plots,i+1)\n",
    "    plt.imshow(A2[i,:,:])\n",
    "    plt.title(f'A2 at {i}')\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "show_active(A1,'A1 Mean over channels')\n",
    "plt.subplot(1,2,2)\n",
    "show_active(A2,'A2 Mean over channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "w9VvsCBKlWYO",
    "outputId": "415a1c24-a7c5-47d1-f12e-3588c849b31f"
   },
   "outputs": [],
   "source": [
    "# calculate gram matrix (vectorized spatially) and show it\n",
    "# for each activations\n",
    "def gram(x):\n",
    "    tmp = x.reshape((channels,hw*hw))\n",
    "    return tmp @ tmp.T / (channels*hw*hw)\n",
    "\n",
    "def show_gram(A,title=''):\n",
    "    plt.figure()\n",
    "    G = gram(A)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(G)\n",
    "    plt.title('Gram of '+title)\n",
    "    plt.subplot(1,2,1)\n",
    "    show_active(A,title)\n",
    "    return G\n",
    "    \n",
    "\n",
    "G_A1 = show_gram(A1,'A1')\n",
    "G_A2 = show_gram(A2,'A2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZwKUaS3IlZIa",
    "outputId": "2e9f6869-3d64-4a9f-c6cd-24db0477a581"
   },
   "outputs": [],
   "source": [
    "def whiten_cov(A):\n",
    "    Avec = A.reshape((channels,hw*hw))\n",
    "    AA = Avec @ Avec.T\n",
    "    # notice that the SVD is of the covariance matrix\n",
    "    u,s,vt = np.linalg.svd(AA,full_matrices=False)\n",
    "    \n",
    "    A_white = ((u @ np.diag(s**(-0.5)) @ vt) @ Avec) # no usage of the singular values here\n",
    "    return A_white, (u, s, vt), Avec\n",
    "    \n",
    "\n",
    "A1_white, A1SVDcov, A1vec = whiten_cov(A1)\n",
    "A2_white, A2SVDcov, A2vec = whiten_cov(A2)\n",
    "    \n",
    "u1, s1, _ = A1SVDcov\n",
    "u2, s2, _ = A2SVDcov\n",
    "\n",
    "# -------A1-------\n",
    "# Apply inverse reconstruction to get colored version\n",
    "A1_mixed_color = u2 @ np.diag(s2**(0.5)) @ u2.T @ A1_white\n",
    "A1_mixed_color = A1_mixed_color.reshape((channels,hw,hw))\n",
    "\n",
    "# -------A2-------\n",
    "# Apply inverse reconstruction to get colored version\n",
    "A2_mixed_color = u1 @ np.diag(s1**(0.5)) @ u1.T @ A2_white\n",
    "A2_mixed_color = A2_mixed_color.reshape((channels,hw,hw))\n",
    "\n",
    "diffA1 = np.mean((A1_mixed_color-A1)**2)\n",
    "diffA2 = np.mean((A2_mixed_color-A2)**2)\n",
    "\n",
    "# show originals\n",
    "# and colored versions\n",
    "G_A1 = show_gram(A1,'A1')\n",
    "_ = show_gram(A1_white.reshape((channels,hw,hw)),'A1_white')\n",
    "G_A1asA2 = show_gram(A1_mixed_color,'A1_mixed_color')\n",
    "\n",
    "# show originals\n",
    "# and colored versions\n",
    "G_A1 = show_gram(A2,'A2')\n",
    "_ = show_gram(A2_white.reshape((channels,hw,hw)),'A2_white')\n",
    "G_A2asA1 = show_gram(A2_mixed_color,'A2_mixed_color')\n",
    "\n",
    "print('A1 average content difference:', diffA1)\n",
    "print('A2 average content difference:', diffA2)\n",
    "\n",
    "print('A1 colored average Grammian difference:', np.mean((G_A2-G_A1asA2)**2))\n",
    "print('A2 colored average Grammian difference:', np.mean((G_A1-G_A2asA1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAvFBYsXlssm"
   },
   "source": [
    "## New implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtyjBIeImmiu"
   },
   "source": [
    "Using the closed form solution of the transformation matrix is:\n",
    "\n",
    "$$T = \\Sigma_c^{-1/2}(\\Sigma_c^{1/2}\\Sigma_s\\Sigma_c^{1/2})^{1/2}\\Sigma_c^{-1/2}$$\n",
    "\n",
    "Useful link: https://stats.stackexchange.com/questions/363425/raising-a-variance-covariance-matrix-to-a-negative-half-power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1pnvsqLlcrO"
   },
   "outputs": [],
   "source": [
    "def matrix_to_power(matrix, power):\n",
    "    # Assuming real symmetric\n",
    "    e_val, e_vec = np.linalg.eigh(matrix)\n",
    "    lambda_ = np.diag(e_val**power)\n",
    "\n",
    "    return e_vec@lambda_@e_vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r10Xptjzhit"
   },
   "outputs": [],
   "source": [
    "def new_transform(content_img, style_img):\n",
    "    sigma_c = np.cov(content_img)\n",
    "    sigma_s = np.cov(style_img)\n",
    "\n",
    "    simga_c_neg_half = matrix_to_power(sigma_c, -1/2)\n",
    "    sigma_c_half = matrix_to_power(sigma_c, 1/2)\n",
    "\n",
    "    inner = matrix_to_power(sigma_c_half@sigma_s@sigma_c_half, 1/2)\n",
    "\n",
    "    transform_mat = simga_c_neg_half@inner@simga_c_neg_half\n",
    "    return transform_mat@content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = A1.reshape(channels, hw*hw)\n",
    "style = A2.reshape(channels, hw*hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored = new_transform(content, style)\n",
    "np.count_nonzero(np.isnan(colored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = show_gram(colored.reshape((channels,hw,hw)),'A1 with A2 style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lab2-Style Transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
